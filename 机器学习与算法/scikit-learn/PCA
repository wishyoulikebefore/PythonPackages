PCA（principal component analysis）：主成分分析
旋转数据，根据新特征对解释数据的重要性来选择子集
第一个新坐标轴选择原始数据中方差最大的方向，第二个坐标轴选择和第一个坐标轴正交且最大方差的方向
通过PCA进行降维处理，可以同时获得SVM和决策树的优点
---------------------------------------------------------------------------------------
优点：降低数据的复杂性，识别最重要的多个特征
缺点：不一定需要，且可能损失有用信息
对于表示同一类数据样本的共同特征是非常有效的，但不适合用于区分不同的样本类
---------------------------------------------------------------------------------------

sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0, iterated_power=’auto’, random_state=None)
-------------------------------------------------------------------------------
>>> 参数
n_components : 保留component的数目
copy : 默认为True，保留原始数据
whiten : （白化）默认为False
svd_solver : 奇异值分解  {‘auto’, ‘full’, ‘arpack’, ‘randomized’}
tol : 默认值为0，svd_solver == ‘arpack’时奇异值的tolerance
iterated_power : 默认值为‘auto’，svd_solver == ‘randomized’时的循环数
random_state : 当svd_solver == ‘arpack’ or ‘randomized'时使用的随机状态
-------------------------------------------------------------------------------
参考资料：
PCA：https://blog.csdn.net/zhongkelee/article/details/44064401
协方差矩阵：https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5

PCA算法
1)去除平均值
2)计算协方差矩阵
3)计算协方差矩阵的特征值及特征向量
4)将特征向量按对应特征值大小排序，保留最上面的N个特征向量
5)将数据转换到上述N个特征向量构建的新空间中
