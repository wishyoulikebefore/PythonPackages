PCA（principal component analysis）：主成分分析
是一种旋转数据集的方法，根据新特征对解释数据的重要性来选择子集
将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性
对于表示同一类数据样本的共同特征是非常有效的，但不适合用于区分不同的样本类

sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0, iterated_power=’auto’, random_state=None)
-------------------------------------------------------------------------------
>>> 参数
n_components : 保留component的数目
copy : 默认为True，保留原始数据
whiten : （白化）默认为False
svd_solver : 奇异值分解  {‘auto’, ‘full’, ‘arpack’, ‘randomized’}
tol : 默认值为0，svd_solver == ‘arpack’时奇异值的tolerance
iterated_power : 默认值为‘auto’，svd_solver == ‘randomized’时的循环数
random_state : 当svd_solver == ‘arpack’ or ‘randomized'时使用的随机状态
-------------------------------------------------------------------------------
参考资料：
PCA：https://blog.csdn.net/zhongkelee/article/details/44064401
协方差矩阵：https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5

PCA算法
将原始数据按列组成n行m列矩阵X
1)将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值。
2)求出协方差矩阵C=XXT/m -- np.cov(scaled_x,scaled_y)
3)求出协方差矩阵的特征值及对应的特征向量 - eig_val, eig_vec = np.linalg.eig(cov)
4)将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P
5)Y=PX即为降维到k维后的数据

需要补的知识：特征值和特征向量的求解原理
