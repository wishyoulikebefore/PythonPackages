为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性

最典型的就是数据的归一化处理：使预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响
如果不进行归一化，那么由于特征向量中不同特征的取值相差较大，会导致目标函数变“扁”，导致在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，训练时间过长；
如果进行归一化以后，目标函数会呈现比较“圆”，这样训练速度大大加快

归一化有如下好处：
1）归一化后加快了梯度下降求最优解的速度
2）归一化有可能提高精度（如KNN）

归一化的方法
1）最大最小标准化（Min-Max Normalization）：使结果值映射到[0 ，1]之间
x-min/max-min
比较适用在数值比较集中的情况
如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量来替代max和min
适用于不涉及距离度量、协方差计算、数据不符合正态分布的时候

2）Z-score标准化方法:数据处理后符合标准正态分布，即均值为0，标准差为1
x-μ/σ
要求原始数据的分布可以近似为高斯分布
适用于：使用距离来度量相似性或者使用PCA技术进行降维的时候

3）非线性归一化：数据分化比较大的场景
log对数函数转换
atan反正切函数转换
#如果想映射的区间为[0，1]，则数据都应该大于等于0，小于0的数据将被映射到[－1，0]区间上
L2范数归一化：特征向量中每个元素均除以向量的L2范数

应用场景：
1）概率模型不需要归一化，因为这种模型不关心变量的取值，而是关心变量的分布和变量之间的条件概率；
2）SVM、线性回归之类的最优化问题需要归一化
3）神经网络需要标准化处理，一般变量的取值在-1到1之间，这样做是为了弱化某些变量的值较大而对模型产生影响
4）在K近邻算法中，如果不对解释变量进行标准化，那么具有小数量级的解释变量的影响就会微乎其微


